# RAG from Scratch

A research paper Q&A system built piece by piece â€” no LangChain, no LlamaIndex.

## Setup

```bash
uv sync
```

That's it. `uv` handles the virtualenv, dependencies, everything.

## Quick Start

```bash
# Grab a paper
wget https://arxiv.org/pdf/1706.03762 -O papers/attention.pdf

# Parse and see structure
uv run rag-parse papers/attention.pdf

# Chunk with all 3 strategies and compare
uv run rag-chunk papers/attention.pdf

# Build index, compare dense vs sparse retrieval
uv run rag-index papers/attention.pdf
```

## Project Structure

```
rag-from-scratch/
â”œâ”€â”€ pyproject.toml          # Dependencies, CLI scripts
â”œâ”€â”€ papers/                 # Drop PDFs here
â”œâ”€â”€ index/                  # Saved FAISS index (auto-created)
â””â”€â”€ src/rag/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ pdf_parser.py       # Module 1: PDF â†’ structured sections
    â”œâ”€â”€ chunkers.py         # Module 1: Three chunking strategies
    â”œâ”€â”€ compare.py          # Module 1: Run & compare chunkers
    â”œâ”€â”€ embedder.py         # Module 2: Text â†’ vectors
    â”œâ”€â”€ vector_store.py     # Module 2: FAISS index + search
    â”œâ”€â”€ sparse.py           # Module 2: BM25 keyword search
    â””â”€â”€ index_and_search.py # Module 2: Build index & compare retrieval
```

## Modules

| Module | Status | What it covers |
|--------|--------|----------------|
| 1. Chunking | âœ… | PDF parsing, fixed/semantic/hierarchical chunking |
| 2. Embedding | âœ… | Vector encoding, FAISS index, dense vs sparse (BM25) |
| 3. Retrieval | ðŸ”œ | Hybrid search, reranking |
| 4. Generation | ðŸ”œ | LLM prompting, citation grounding, hallucination |
| 5. Evaluation | ðŸ”œ | Retrieval metrics, answer quality, eval framework |

## Why from Scratch?

Frameworks like LangChain hide the decisions that matter. When your RAG gives wrong answers, you need to know: is it the chunking? The embedding model? The retrieval? The prompt? Building each piece yourself means you understand the full chain.